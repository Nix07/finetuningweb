<!doctype html>
<html lang="en">

<head>
  <title>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta name="description"
    content="Mechansitic investigation on how fine-tuning augments large language models' capabilities." />
  <meta property="og:title" content="Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking" />
  <meta property="og:description"
    content="Mechansitic investigation on how fine-tuning augments large language models' capabilities." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking" />
  <meta name="twitter:description"
    content="Mechansitic investigation on how fine-tuning augments large language models' capabilities." />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">

  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
    integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Math&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">

  <style>
    .relatedthumb {
      float: left;
      width: 200px;
      margin: 3px 10px 7px 0;
    }

    .relatedblock {
      clear: both;
      display: inline-block;
    }

    .bold-sc {
      font-variant: small-caps;
      font-weight: bold;
    }

    .cite,
    .citegroup {
      margin-bottom: 8px;
    }

    :target {
      background-color: yellow;
    }
  </style>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
  </script>

</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <h1 class="lead">
        Fine-Tuning Enhances Existing Mechanisms:<br>A Case Study on Entity Tracking
      </h1>
      <address>
        <nobr><a href="https://nix07.github.io/" target="_blank">Nikhil Prakash</a><sup>1</sup>,</nobr>
        <nobr><a href="https://tamarott.github.io/" target="_blank">Tamar Rott Shaham</a><sup>2</sup>,</nobr>
        <nobr><a href="https://il.linkedin.com/in/tal-haklay-501032192" target="_blank">Tal Haklay</a><sup>3</sup>,
        </nobr>
        <nobr><a href="https://belinkov.com/" target="_blank">Yonatan Belinkov</a><sup>3</sup>,</nobr>
        <nobr><a href="https://baulab.info/" target="_blank">David Bau</a><sup>1</sup></nobr>
        <br>
        <nobr><sup>1</sup><a href="https://khoury.northeastern.edu/" target="_blank">Northeastern University</a>,</nobr>
        <nobr><sup>2</sup><a href="https://www.csail.mit.edu/" target="_blank">MIT CSAIL</a></nobr>
        <nobr><sup>3</sup><a href="https://www.cs.technion.ac.il/" target="_blank">Technion - IIT</a></nobr>;

      </address>
    </div>
  </div><!-- end nd-pageheader -->

  <div class="container">
    <div class="row justify-content-center text-center">

      <p>
        <a href="https://openreview.net/forum?id=8sKcAWOf2D" class="d-inline-block p-3 align-top" target="_blank"><img
            height="100" width="78" src="images/paper-thumb.png" style="border:1px solid; margin: 0 38px;"
            alt="ArXiv Preprint thumbnail" data-nothumb=""><br>ArXiv<br>Preprint</a>
        <a href="https://github.com/Nix07/finetuning" class="d-inline-block p-3 align-top" target="_blank"><img
            height="100" width="78" src="images/code-thumb.png" style="border:1px solid; margin: 0 38px;"
            alt="Github code thumbnail" data-nothumb=""><br>Source
          Code<br>Github</a>
        <a href="https://huggingface.co/nikhil07prakash/float-7b" class="d-inline-block p-3 align-bottom"
          target="_blank"><img height="78" width="104" src="images/huggingface-thumb.png" style="border:1px solid"
            alt="Huggingface demo thumbnail" data-nothumb=""><br>Fine-Tuned<br>Model Weights</a>
      </p>
      </p>

      <div class="card" style="max-width: 1020px;">
        <div class="card-block">
          <h3>How Does Fine-Tuning Improve Language Models' Capabilities?</h3>
          <p style="text-align: justify;">
            Fine-tuning has become a ubiquitous technique for enhancing language models' capabilities across diverse
            tasks. However, the mechanistic updates underpinning these performance gains remain poorly understood. Does
            the fine-tuned model employ the same mechanism, or does it modify its algorithm to address a task? Are the
            same set of model components (i.e. circuit) engaged in performing the task, or are
            different ones involved?
          </p>
          <p style="text-align: justify;">
            <b>Our findings suggest that fine-tuning enhances, rather than fundamentally alters, the mechanistic
              operation of the model</b>. Specifically, we observed that the task of entity tracking in Llama-7B and its
            fine-tuned variants is primarily performed by the same circuit. Not only are the same set of components
            involved, but their functionality also remains consistent across the models. Finally, we were
            able to attribute the performance gap to an improved sub-mechansim in the fine-tuned models.
          </p>
        </div><!--card-block-->
      </div><!--card-->

    </div><!--row-->

    <div class="row">
      <div class="col">

        <h2>Is the Same Circuit Present After Fine-Tuning? </h2>
        <p>
          We employ Path Patching to identify the circuit for entity tracking in Llama-7B, comprising four head groups,
          as shown in Figure 1. Group D collects segment information at previous query label token and passes it
          to Group C via V-composition. The output of Group C is then conveyed to the last token residual stream through
          V-composition with Group B, and utilized by Group A heads via Q-composition to attend to the correct object
          token.
        </p>

        <figure class="center_image" style="margin-top: 30px">
          <center><img src="images/paper/circuit_flow.png" style="width:100%; max-width:800px"></center>
          <figcaption>
            Figure 1: <b>Entity Tracking circuit</b> (Cir). The circuit is composed of 4 groups of heads
            (A,B,C,D) located at the last token (A,B), query label (C), and previous query label (D) token positions.
            Each group is illustrated by a prominent head in that group.
          </figcaption>
        </figure>

        <p>
          Surprisely, we found that the <b>same circuit is primarily responsible for performing entity tracking in
            both the base and fine-tuned models</b>. Specifically, Vicuna-7B utilizes roughly the same circuit as that
          of Llama-7B to perform entity tracking. Whereas, in Goat-7B and FLoat-7B, the same circuit is present, but
          with additional components.
        </p>

        <table class="table table-bordered">
          <tr>
            <th></th>
            <th></th>
            <th colspan="3" style="text-align: center;">Accuracy</th>
          </tr>
          <tr>
            <th>Model</th>
            <th style="text-align: center;">Finetuned?</th>
            <th style="text-align: center;">Full-Model</th>
            <th style="text-align: center;">Circuit</th>
            <th style="text-align: center;">Random Circuit</th>
            <th style="text-align: center;">Faithfulness</th>
          </tr>
          <tr>
            <td>Llama-7B</td>
            <td style="text-align: center;">--</td>
            <td style="text-align: center;">0.66</td>
            <td style="text-align: center;">0.66</td>
            <td style="text-align: center;">0.00</td>
            <td style="text-align: center;">1.00</td>
          </tr>
          <tr>
            <td>Vicuna-7B</td>
            <td style="text-align: center;">User conversations</td>
            <td style="text-align: center;">0.67</td>
            <td style="text-align: center;">0.65</td>
            <td style="text-align: center;">0.00</td>
            <td style="text-align: center;">0.97</td>
          </tr>
          <tr>
            <td>Goat-7B</td>
            <td style="text-align: center;">Arithmetic tasks (LoRA)</td>
            <td style="text-align: center;">0.82</td>
            <td style="text-align: center;">0.73</td>
            <td style="text-align: center;">0.01</td>
            <td style="text-align: center;">0.89</td>
          </tr>
          <tr>
            <td>FLoat-7B</td>
            <td style="text-align: center;">Arithmetic tasks (w/o LoRA)</td>
            <td style="text-align: center;">0.82</td>
            <td style="text-align: center;">0.72</td>
            <td style="text-align: center;">0.01</td>
            <td style="text-align: center;">0.88</td>
          </tr>
        </table>

        <h2>Is Circuit Functionality the Same After Fine-Tuning?</h2>
        <p>
          We employ Desiderata-based Component Masking (DCM) to discern the functionality of circuit components. For
          more details on the method, please refer to <a href="https://dcm.baulab.info/">dcm.baulab.info</a>.
        </p>
        <figure class="center_image" style="margin-top: 30px">
          <center><img src="images/paper/DCM.png" style="width:100%; max-width:800px"></center>
          <figcaption>
            Figure 2: <b>Circuit Functionality in Llama-7B, Vicuna-7B, Goat-7B, and FLoat-7B</b>. We use DCM
            to uncover the functionality of each subgroup of the circuit. Group A (pink) is mainly sensitive to the
            value desideratum, while groups B, C (purple, turquoise) are responsible for positional information.
            We find group D insensitive to each of the three desideratum. Error bars indicate standard deviation.
          </figcaption>
        </figure>
        <p>
          We discovered that the <b>functionality of the circuit components remains consistent across the models</b>.
          Specifically, Group A primarily encodes the value of the correct object, while Groups B and C are responsible
          for encoding positional information. Additionally, we observed that Group D is insensitive to each of the
          three functionalities.
        </p>
        <p>
          The observations that the same circuit is primarily responsible for performing entity tracking in both the
          base and fine-tuned models, and that the functionality of the circuit components remains consistent across the
          models, indicates that the <b>fine-tuned models leverage the same mechanism as the base model</b> to perform
          entity tracking. However, increased performance of fine-tuned models suggest that fine-tuning
          enhances that existing mechanism.
        </p>

        <h2>Why Do Goat-7B and FLoat-7B Perform Better?</h2>
        <p>
          To attribute the performance gap between fine-tuned models and Llama-7B to a specific sub-mechanism, we
          propose a
          novel method <b>Cross-Model Activation Patching</b> (CMAP). Unlike naive activation patching which
          involves patching activations of the same model on different inputs, CMAP requires patching activations of the
          same components of different models on the same input. More specifically, we patch the output of heads in
          Goat-7B circuit from the Goat-7B model to Llama-7B model, to identify which step in Goat-7B model has improved
          compared to Llama-7B. Similarly, we perform the same patching process for heads in the FLoat-7B circuit.
        </p>
        <figure class="center_image" style="margin-top: 30px">
          <center><img src="images/paper/CMAP.png" style="width:100%; max-width:800px"></center>
          <figcaption>
            Figure 3: <b>Why do Goat-7B and FLoat-7B perform better?</b> We use CMAP to patch activations of the Goat-7B
            and FLoat-7B circuit components, from Goat-7B and FLoat-7B to Llama-7B model respectively, to attribute the
            performance improvement to a specific sub-mechanism used to perform entity tracking tasks. We patch the
            output of the subset of heads in each group that are involved in the primary functionality. We find that
            patching Value Fetcher heads can solely improve the performance of Llama-7B to that of Goat-7B and FLoat-7B.
            Additionally, we also observe a significant performance boost when the output of Position Transmitter heads
            is patched.
          </figcaption>
        </figure>
        <p>
          We note that the activations of fine-tuned models are compatible with those of the base model, despite the
          possibility of using entirely different subspaces and/or norms to encode information. Furthermore, our
          findings indicate that the Value Fetcher Heads in fine-tuned models encode an improved representation of the
          correct object, consistent with the results from the previous experiment. Additionally, we observe that the
          Position Transmitter Heads transmit augmented positional information to the Value Fetcher Heads.
        </p>

        <h2>Related Works</h2>
        <p>Our work builds upon insights from previous research that has investigated large language models from various
          other perspectives:
        </p>
        <h3>Entity Tracking</h3>
        <p class="citation">
          <a href="https://arxiv.org/pdf/2305.02363.pdf" target="_blank">
            <img src="images/related_works/entity_tracking.png" alt="Entity Tracking (Kim et. al 2023)">
            Najoung Kim, Sebastian Schuster. Entity Tracking in Language Models. arXiv preprint 2023.
          </a>
          <br>
          <b>Notes:</b> Investigated whether large language models can track the states of entities through a discourse,
          and
          finds that models trained on both text and code exhibit better entity tracking abilities than models trained
          solely on text.
        </p>

        <p class="citation">
          <a href="https://arxiv.org/pdf/2106.00737.pdf" target="_blank">
            <img src="images/related_works/entity_state.png" alt="Implicit Meaning (Li et. al 2021)">
            Belinda Z. Li, Maxwell Nye, Jacob Andreas. Implicit Representations of Meaning in Neural Language Models.
            arXiv preprint 2021.
          </a>
          <br>
          <b>Notes:</b> Investigated whether neural language models implicitly encode representations of meaning without
          being explicitly trained to do so.
        </p>

        <h3>Transformer Interpretability</h3>
        <p class="citation">
          <a href="https://arxiv.org/pdf/2211.00593.pdf" target="_blank">
            <img src="images/related_works/IOI.png" alt="Path Patching (Wang et. al 2022)">
            Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt. Interpretability in the
            Wild: a Circuit for Indirect Object Identification in GPT-2 small. arXiv preprint 2022.
          </a>
          <br>
          <b>Notes:</b> Proposed Path Patching algorithm to reverse engineer the circuit responsible for performing
          Indirect Object Identification (IOI) task in a GPT2-small.
        </p>

        <p class="citation">
          <a href="https://arxiv.org/pdf/2307.03637.pdf" target="_blank">
            <img src="images/related_works/DCM.png" alt="DCM (Davies et. al 2023)">
            Xander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, David Bau. Discovering Variable Binding
            Circuitry with Desiderata. arXiv preprint 2023.
          </a>
          <br>
          <b>Notes:</b> Proposed Desiderata-based Component Masking (DCM) method to localize components responsible for
          variable binding in Llama-13B.
        </p>

        <p class="citation">
          <a href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank">
            <img src="images/related_works/elhage-2021.png" alt="Transformer_circuits (Elhage et. al 2021)">
            Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao
            Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,
            Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,
            Sam McCandlish, Chris Olah. A Mathematical Framework for Transformer Circuits. Anthropic 2021.
          </a>
          <br>
          <b>Notes:</b> Analyzes internal mechanisms of transformer components, developing mathematical tools for
          understanding patterns of computations. Observes information-copying behavior in self-attention and implicates
          it in the strong performance of transformers.
        </p>

        <h2>Complimentary Works</h2>
        <p class="citation">
          <a href="https://arxiv.org/pdf/2311.12786.pdf" target="_blank">
            <img src="images/related_works/finetuning.png" alt="Finetuning (Jain et. al 2023)">
            Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim
            Rockt√§schel, David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on procedurally
            defined tasks. arXiv preprint (2023).
          </a>
          <br>
          <b>Notes:</b> Investigated the impact of fine-tuning on LLMs from a mechanistic perspective. Although their
          main
          finding, suggesting that fine-tuning minimally alters pretrained capabilities, resonates with our result
          of enhancing existing mechanisms through fine-tuning, their study involved controlled experiments
          utilizing transformer models created using the <i>tracr library</i>. In contrast, our
          experiments focus on established LLMs such as Llama-7B and their fine-tuned variants, specifically
          in the context of entity tracking tasks, which we believe better represent real-world language tasks.
        </p>

        <p class="citation">
          <a href="https://arxiv.org/pdf/2310.17191.pdf" target="_blank">
            <img src="images/related_works/binding.png" alt="Binding (Feng et. al 2023)">
            Jiahai Feng, Jacob Steinhardt. How do Language Models Bind Entities in Context? arXiv preprint (2023).
          </a>
          <br>
          <b>Notes:</b> Investigated how LLMs keep track of various properties associated with an entity. Their findings
          indicated that models generate binding ID vectors corresponding to entities and attributes. We find it
          intriguing to further investigate the interaction between these binding ID vectors and the entity tracking
          circuit we have identified.
        </p>

        <h2>How to cite</h2>
        TBD

        <!-- <p>The preprint can be cited as follows.
        </p>

        <div class="card">
          <h3 class="card-header">bibliography</h3>
          <div class="card-block">
            <p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
              Xander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, David Bau.
              <em>Discovering Variable Binding Circuitry with Desiderata.</em>" arXiv preprint <nobr><a
                  href="https://arxiv.org/abs/2307.03637">arXiv:2307.03637</a> (2023).</nobr>
            </p>
          </div>
          <h3 class="card-header">bibtex</h3>
          <div class="card-block">
            <pre class="card-text clickselect">
@article{davies2023discovering,
  title={Discovering Variable Binding Circuitry with Desiderata},
  author={Davies, Xander and Max Nadeau and Nikhil Prakash and Tamar Rott Shaham and David Bau},
  journal={arXiv preprint arXiv:2307.03637},
  year={2023}
}
</pre>
          </div>
        </div> -->
        </p>

      </div>
    </div><!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://baulab.info/">About the Bau Lab</a>
      </div>
    </div>
  </footer>

</body>
<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
</script>

</html>